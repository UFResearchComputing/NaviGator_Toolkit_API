{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff19f495-59af-474f-a381-d7d78ad4a570",
   "metadata": {},
   "source": [
    "\n",
    "# NaviGator Toolkit File Upload API Demo\n",
    "\n",
    "## NOTE: This only works with cloud models, not locally hosted models.\n",
    "\n",
    "> As of early 2026, the NaviGator Toolkit API supports file uploads for certain cloud models. This allows you to provide additional context or data to the model when making requests. **Locally hosted models do not currently support file uploads**, so this feature is only available when using cloud-based models through the NaviGator API.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "This notebook demonstrates uploading a file using the NaviGator Toolkit API ([see here for more information](https://it.ufl.edu/ai/navigator-toolkit/)). File uploads only work with certain cloud models; locally hosted models do not currently support uploads.\n",
    "\n",
    "You will need a NaviGator API key. Store the key in a `.json` file with the following format:\n",
    "\n",
    "    {\n",
    "      \"OPENAI_API_KEY\" : \"Put your key here in the quotes\",\n",
    "      \"base_url\" : \"https://api.ai.it.ufl.edu/\"\n",
    "    }\n",
    "\n",
    "We recommend storing the file in your home directory to reduce the chance of accidentally committing it to a git repo. Anyone with your API key can use NaviGator as you! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba376d4-d4f1-4e48-a7c0-3ca4a7c144c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "import base64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0742a36-ccf5-4d0c-9c71-f2ba7b7735cb",
   "metadata": {},
   "source": [
    "## Load `.json` file with your key and API endpoint URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1742595-a563-4065-be5a-2b08107bdbe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the path to your jsnkey file\n",
    "key_file = '/home/magitz/navigator_api_keys.json'\n",
    "\n",
    "\n",
    "# Load the JSON file\n",
    "with open(key_file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract the values\n",
    "OPENAI_API_KEY = data.get('OPENAI_API_KEY')\n",
    "base_url = data.get('base_url')\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['TOOLKIT_API_KEY'] = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05900196-4d7d-4dea-8b6a-9c119bfd3035",
   "metadata": {},
   "source": [
    "## Test connectivity and get model list\n",
    "\n",
    "The reply should list the models available with your API key. An example (truncated) output is:\n",
    "\n",
    "    SyncPage[Model](data=[Model(id='llama-3.1-70b-instruct', created=1677610602, object='model', owned_by='openai'), Model(id='sfr-embedding-mistral', created=1677610602, object='model', owned_by='openai'),...)], object='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea3baa5-eea7-46f3-aa91-753907b44ad7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncPage[Model](data=[Model(id='llama-3.1-70b-instruct', created=1677610602, object='model', owned_by='openai'), Model(id='llama-3.1-8b-instruct', created=1677610602, object='model', owned_by='openai'), Model(id='llama-3.1-nemotron-nano-8B-v1', created=1677610602, object='model', owned_by='openai'), Model(id='llama-3.3-70b-instruct', created=1677610602, object='model', owned_by='openai'), Model(id='mistral-7b-instruct', created=1677610602, object='model', owned_by='openai'), Model(id='mistral-small-3.1', created=1677610602, object='model', owned_by='openai'), Model(id='codestral-22b', created=1677610602, object='model', owned_by='openai'), Model(id='gemma-3-27b-it', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-oss-20b', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-oss-120b', created=1677610602, object='model', owned_by='openai'), Model(id='granite-3.3-8b-instruct', created=1677610602, object='model', owned_by='openai'), Model(id='sfr-embedding-mistral', created=1677610602, object='model', owned_by='openai'), Model(id='nomic-embed-text-v1.5', created=1677610602, object='model', owned_by='openai'), Model(id='flux.1-dev', created=1677610602, object='model', owned_by='openai'), Model(id='flux.1-schnell', created=1677610602, object='model', owned_by='openai'), Model(id='whisper-large-v3', created=1677610602, object='model', owned_by='openai'), Model(id='kokoro', created=1677610602, object='model', owned_by='openai'), Model(id='o1', created=1677610602, object='model', owned_by='openai'), Model(id='o3', created=1677610602, object='model', owned_by='openai'), Model(id='o3-mini', created=1677610602, object='model', owned_by='openai'), Model(id='o3-mini-high', created=1677610602, object='model', owned_by='openai'), Model(id='o3-mini-medium', created=1677610602, object='model', owned_by='openai'), Model(id='o4-mini', created=1677610602, object='model', owned_by='openai'), Model(id='o4-mini-high', created=1677610602, object='model', owned_by='openai'), Model(id='o4-mini-medium', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-4.1', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-4.1-mini', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-4.1-nano', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-4o', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-4o-mini', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-5', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-5-mini', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-5-nano', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-5.1', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-5.1-codex', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-5.1-codex-mini', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-5.2', created=1677610602, object='model', owned_by='openai'), Model(id='gemini-2.0-flash', created=1677610602, object='model', owned_by='openai'), Model(id='gemini-2.5-flash', created=1677610602, object='model', owned_by='openai'), Model(id='gemini-2.5-pro', created=1677610602, object='model', owned_by='openai'), Model(id='claude-3.5-haiku', created=1677610602, object='model', owned_by='openai'), Model(id='claude-3.5-sonnet', created=1677610602, object='model', owned_by='openai'), Model(id='claude-3.5-sonnet-v2', created=1677610602, object='model', owned_by='openai'), Model(id='claude-3.7-sonnet', created=1677610602, object='model', owned_by='openai'), Model(id='claude-3.7-sonnet-thinking', created=1677610602, object='model', owned_by='openai'), Model(id='claude-3-haiku', created=1677610602, object='model', owned_by='openai'), Model(id='claude-3-opus', created=1677610602, object='model', owned_by='openai'), Model(id='claude-4-sonnet', created=1677610602, object='model', owned_by='openai'), Model(id='claude-4.5-sonnet', created=1677610602, object='model', owned_by='openai'), Model(id='claude-4.5-sonnet-thinking', created=1677610602, object='model', owned_by='openai'), Model(id='nova-lite', created=1677610602, object='model', owned_by='openai'), Model(id='nova-micro', created=1677610602, object='model', owned_by='openai'), Model(id='nova-pro', created=1677610602, object='model', owned_by='openai'), Model(id='mistral-large', created=1677610602, object='model', owned_by='openai'), Model(id='mistral-large-2', created=1677610602, object='model', owned_by='openai'), Model(id='mistral-small', created=1677610602, object='model', owned_by='openai')], object='list')\n"
     ]
    }
   ],
   "source": [
    "# Check list of available models\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"TOOLKIT_API_KEY\"),\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "if not hasattr(client, \"responses\"):\n",
    "    raise RuntimeError(\n",
    "        \"OpenAI SDK is missing the responses API. \"\n",
    "        \"Upgrade to a newer openai package (e.g., pip install -U openai) \"\n",
    "        \"or use a kernel/environment that includes responses.\"\n",
    "    )\n",
    "\n",
    "response = client.models.list()\n",
    " \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5718c829-bc94-454c-bbe8-1743465430c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama-3.1-70b-instruct\n",
      "llama-3.1-8b-instruct\n",
      "llama-3.1-nemotron-nano-8B-v1\n",
      "llama-3.3-70b-instruct\n",
      "mistral-7b-instruct\n",
      "mistral-small-3.1\n",
      "codestral-22b\n",
      "gemma-3-27b-it\n",
      "gpt-oss-20b\n",
      "gpt-oss-120b\n",
      "granite-3.3-8b-instruct\n",
      "sfr-embedding-mistral\n",
      "nomic-embed-text-v1.5\n",
      "flux.1-dev\n",
      "flux.1-schnell\n",
      "whisper-large-v3\n",
      "kokoro\n",
      "o1\n",
      "o3\n",
      "o3-mini\n",
      "o3-mini-high\n",
      "o3-mini-medium\n",
      "o4-mini\n",
      "o4-mini-high\n",
      "o4-mini-medium\n",
      "gpt-4.1\n",
      "gpt-4.1-mini\n",
      "gpt-4.1-nano\n",
      "gpt-4o\n",
      "gpt-4o-mini\n",
      "gpt-5\n",
      "gpt-5-mini\n",
      "gpt-5-nano\n",
      "gpt-5.1\n",
      "gpt-5.1-codex\n",
      "gpt-5.1-codex-mini\n",
      "gpt-5.2\n",
      "gemini-2.0-flash\n",
      "gemini-2.5-flash\n",
      "gemini-2.5-pro\n",
      "claude-3.5-haiku\n",
      "claude-3.5-sonnet\n",
      "claude-3.5-sonnet-v2\n",
      "claude-3.7-sonnet\n",
      "claude-3.7-sonnet-thinking\n",
      "claude-3-haiku\n",
      "claude-3-opus\n",
      "claude-4-sonnet\n",
      "claude-4.5-sonnet\n",
      "claude-4.5-sonnet-thinking\n",
      "nova-lite\n",
      "nova-micro\n",
      "nova-pro\n",
      "mistral-large\n",
      "mistral-large-2\n",
      "mistral-small\n"
     ]
    }
   ],
   "source": [
    "# Print available models in better format\n",
    "for model in response:\n",
    "    print(model.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7fa920-4a14-4c73-90ed-c395da6f57f3",
   "metadata": {},
   "source": [
    "## Helper function to convert LiteLLM filename to a usable filename\n",
    "\n",
    "LiteLLM, the API endpoint, encodes the uploaded file in base64 with the model name as part of the string. We need to decode that and strip out just the file name to send to a cloud model. Otherwise, we exceed the name length limits—at least with the Azure-hosted OpenAI models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70c6c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_base64(encoded: str) -> bytes:\n",
    "    \"\"\"Decode a base64-encoded string into raw bytes.\"\"\"\n",
    "    # Add padding if missing\n",
    "    missing_padding = len(encoded) % 4\n",
    "    if missing_padding:\n",
    "        encoded += \"=\" * (4 - missing_padding)\n",
    "    return base64.b64decode(encoded)\n",
    "    \n",
    "def get_file_name(encoded_name, debug=False):\n",
    "    \"\"\"Take a base64-encoded file path and return the file name.\"\"\"\n",
    "\n",
    "    litellmFileBytes = decode_base64(encoded_name.split(\"-\")[1])\n",
    "    if debug:\n",
    "        print(f\"litellmFileBytes: {litellmFileBytes}\")\n",
    "    \n",
    "    litellmFile = litellmFileBytes.decode('utf-8')\n",
    "    if debug:\n",
    "        print(f\"litellmFile: {litellmFile}\")\n",
    "    \n",
    "    file_id = litellmFile.split(\":\")[1].split(\";\")[0]\n",
    "    if debug:\n",
    "        print(f\"Usable file_id is: {file_id}\")\n",
    "\n",
    "    return file_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f0415b",
   "metadata": {},
   "source": [
    "## Load a file\n",
    "\n",
    "In this example, we use the NVIDIA Medium-Range Weather forecast model paper available here: [https://d1qx31qr3h6wln.cloudfront.net/publications/atlas-paper.pdf](https://d1qx31qr3h6wln.cloudfront.net/publications/atlas-paper.pdf). This paper is in the `data` folder of the repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "833cb6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file: data/Kossaifi_et_el_2026_NVIDIA_Medium-Range_Forecast.pdf...\n",
      "✅ Uploaded file (encoded name): file-bGl0ZWxsbTphc3Npc3RhbnQtODR6OTR2QVplNnd4b0J1VjROZDRrdTttb2RlbCxncHQtNG8\n",
      "Decoding file name...\n",
      "✅ Usable file_id is: assistant-84z94vAZe6wxoBuV4Nd4ku\n"
     ]
    }
   ],
   "source": [
    "# Define the file to upload \n",
    "file = \"data/Kossaifi_et_el_2026_NVIDIA_Medium-Range_Forecast.pdf\"\n",
    "\n",
    "# Define the purpose and model\n",
    "purpose = \"assistants\"\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "\n",
    "# Upload a file to the API\n",
    "print(f\"Uploading file: {file}...\")\n",
    "\n",
    "try:\n",
    "    uploaded_file = client.files.create(\n",
    "        file = open(file, \"rb\"),\n",
    "        purpose = purpose,\n",
    "        extra_body = {\n",
    "            \"model\": model\n",
    "        },\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error: Uploading file failed.\")\n",
    "    print(f\"Error: Exception is: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"✅ Uploaded file (encoded name): {uploaded_file.id}\")\n",
    "print(f\"Decoding file name...\")\n",
    "file_id = get_file_name(uploaded_file.id, debug=False)\n",
    "print(f\"✅ Usable file_id is: {file_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef83c4b0",
   "metadata": {},
   "source": [
    "## Create a summary of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3396f37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to create a response...\n",
      "Response is:\n",
      " The document introduces a new framework called ATLAS for medium-range probabilistic weather forecasting. Key findings include:\n",
      "\n",
      "1. **Unified Approach over Complexity**:\n",
      "   - The study demonstrates that state-of-the-art probabilistic skill in weather forecasting does not require complex bespoke architectures or specialized training methods.\n",
      "   - ATLAS leverages a streamlined, scalable design with standard transformer architectures and a history-conditioned local projector, proving effective in capturing high-resolution physics.\n",
      "\n",
      "2. **Method-Agnostic Robustness**:\n",
      "   - The framework support various probabilistic methods such as stochastic interpolants, diffusion models, and CRPS-based ensemble training, performing robustly across different estimation strategies.\n",
      "\n",
      "3. **Performance**:\n",
      "   - ATLAS outperforms the European Centre for Medium-Range Weather Forecasts’ Integrated Forecasting System (IFS) and the deep learning-based model GenCast on most variables.\n",
      "   - Statistically significant improvements are observed, suggesting general-purpose models' scaling is more effective than domain-specific complexity.\n",
      "\n",
      "4. **Efficiency**:\n",
      "   - ATLAS achieves superior performance while operating more efficiently, requiring less computational power and time compared to traditional solvers and other deep learning models.\n",
      "\n",
      "5. **Case Studies and Real-World Application**:\n",
      "   - The framework shows remarkable performance in predicting extreme weather events, such as Storm Dennis, maintaining accurate representation of dynamic atmospheric conditions even during prolonged forecasts.\n",
      "   - In tropical cyclone tracking, ATLAS demonstrates skillful track predictions and outperforms current benchmarks, suggesting improved reliability in real-world applications.\n",
      "\n",
      "6. **Simplified Latent Space Modeling**:\n",
      "   - The use of bilinear downsampling in a latent space modeling approach simplifies the framework’s architecture while retaining high accuracy, avoiding the common pitfalls of variational autoencoder models in atmospheric data.\n",
      "\n",
      "7. **Future Implications**:\n",
      "   - The study suggests that future improvements in weather forecasting models may lie in scaling general-purpose architectures and enhancing training strategies rather than increasing complexity.\n",
      "   - The success in using diffusion-based approaches hints at potential advancements in guided and task-diverse weather forecasting systems.\n",
      "\n",
      "Overall, ATLAS represents a significant advancement in medium-range probabilistic weather forecasting, combining simplicity, scalability, and robustness, potentially setting a new standard for future development in the field.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deine the prompt\n",
    "prompt = \"Summarize the key findings of the document.\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that provides a variety of AI file services.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"input_text\",\n",
    "                \"text\": prompt\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"input_file\",\n",
    "                \"file_id\": file_id\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    print(\"Attempting to create a response...\")\n",
    "    response = client.responses.create(\n",
    "        model = model,\n",
    "        input = messages\n",
    "    )\n",
    "\n",
    "    response_id = response.id\n",
    "\n",
    "    try:\n",
    "        retrieved_response = client.responses.retrieve(response_id)\n",
    "        print(f\"Response is:\\n {retrieved_response.output_text}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve response: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An expection occured while trying to create a chat session: {e}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
